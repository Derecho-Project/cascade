[DERECHO]
# leader ip - the leader's ip address
leader_ip = 127.0.0.1
# leader gms port - the leader's gms port
leader_gms_port = 23580
# leader external client port - the leader's 
leader_external_port = 32645
# my local id - each node should have a different id
local_id = 0
# my local ip address
local_ip = 127.0.0.1
# derecho gms port
gms_port = 23580
# derecho rpc port
state_transfer_port = 28366
# sst tcp port
sst_port = 37683
# rdmc tcp port
rdmc_port = 31675
# external port
external_port = 32645
# this is the frequency of the failure detector thread.
# It is best to leave this to 1 ms for RDMA. If it is too high,
# you run the risk of overflowing the queue of outstanding sends.
heartbeat_ms = 100
# sst poll completion queue timeout in millisecond
sst_poll_cq_timeout_ms = 100
# disable partitioning safety
# By disabling this feature, the derecho is allowed to run when active
# members cannot form a majority. Please be aware of the 'split-brain'
# syndrome:https://en.wikipedia.org/wiki/Split-brain and make sure your
# application is fine with it. 
# To help the user play with derecho at beginning, we disabled the
# partitioning safety. We suggest to set it to false for serious deployment
disable_partitioning_safety = false

# maximum payload size for P2P requests
max_p2p_request_payload_size = 10240
# maximum payload size for P2P replies
max_p2p_reply_payload_size = 10240
# window size for P2P requests and replies
p2p_window_size = 16

# Subgroup configurations
# - The default subgroup settings
[SUBGROUP/DEFAULT]
# maximum payload size
# Any message with size large than this has to be broken
# down to multiple messages.
# Large message consumes memory space because the memory buffers
# have to be pre-allocated.
max_payload_size = 10240
# maximum reply payload size
# This is for replies generated by ordered sends in the subgroup
max_reply_payload_size = 10240
# maximum smc (SST's small message multicast) payload size
# If the message size is smaller or equal to this size,
# it will be sent using SST multicast, otherwise it will
# try RDMC if the message size is smaller than max_payload_size.
max_smc_payload_size = 10240
# block size depends on your max_payload_size.
# It is only relevant if you are ever going to send a message using RDMC.
# In that case, it should be set to the same value as the max_payload_size,
# if the max_payload_size is around 1 MB. For very large messages, the block # size should be a few MBs (1 is fine).
block_size = 1048576
# message window size
# the length of the message pipeline
window_size = 16
# the send algorithm for RDMC. Other options are
# chain_send, sequential_send, tree_send
rdmc_send_algorithm = binomial_send
# - SAMPLE for large message settings
[SUBGROUP/LARGE]
max_payload_size = 102400
max_reply_payload_size = 102400
max_smc_payload_size = 10240
block_size = 10240
window_size = 3
rdmc_send_algorithm = binomial_send
# - SAMPLE for small message settings
[SUBGROUP/SMALL]
max_payload_size = 100
max_reply_payload_size = 100
max_smc_payload_size = 100
# does not matter unless max_payload_size > max_smc_payload_size
block_size = 0
window_size = 50
rdmc_send_algorithm = binomial_send

[SUBGROUP/VCS]
max_payload_size = 8192
max_reply_payload_size = 8192
max_smc_payload_size = 10240
# does not matter unless max_payload_size > max_smc_payload_size
block_size = 1048576
window_size = 50
rdmc_send_algorithm = binomial_send
num_shards = 1
min_nodes = 1
max_nodes = 4

[SUBGROUP/PCS]
max_payload_size = 8192
max_reply_payload_size = 8192
max_smc_payload_size = 10240
# does not matter unless max_payload_size > max_smc_payload_size
block_size = 1048576
window_size = 50
rdmc_send_algorithm = binomial_send
num_shards = 1
min_nodes = 1
max_nodes = 4

# RDMA section contains configurations of the following
# - which RDMA device to use
# - device configurations
[RDMA]
# 1. provider = bgq|gni|mlx|netdir|psm|psm2|rxd|rxm|shm|sockets|udp|usnic|verbs
# possible options(only 'sockets' and 'verbs' providers are tested so far):
# bgq     - The Blue Gene/Q Fabric Provider
# gni     - The GNI Fabric Provider (Cray XC (TM) systems)
# mlx     - The MLX Fabric Provider (UCX library)
# netdir  - The Network Direct Fabric Provider (Microsoft Network Direct SPI)
# psm     - The PSM Fabric Provider
# psm2    - The PSM2 Fabric Provider
# rxd     - The RxD (RDM over DGRAM) Utility Provider
# rxm     - The RxM (RDM over MSG) Utility Provider
# shm     - The SHM Fabric Provider
# sockets - The Sockets Fabric Provider (TCP)
# udp     - The UDP Fabric Provider
# usnic   - The usNIC Fabric Provider (Cisco VIC)
# verbs   - The Verbs Fabric Provider
provider = sockets

# 2. domain
# For sockets provider, domain is the NIC name (ifconfig | grep -v -e "^ ")
# For verbs provider, domain is the device name (ibv_devices)
domain = lo

# 3. tx_depth 
# tx_depth applies to hints->tx_attr->size, where hint is a struct fi_info object.
# see https://ofiwg.github.io/libfabric/master/man/fi_getinfo.3.html
tx_depth = 256

# 4. rx_depth:
# rx_depth applies to hints->rx_attr->size, where hint is a struct fi_info object.
# see https://ofiwg.github.io/libfabric/master/man/fi_getinfo.3.html
rx_depth = 256

# Persistent configurations
[PERS]
# persistent directory for file system-based logfile.
file_path = .plog
ramdisk_path = /dev/shm/volatile_t
# Reset persistent data
# CAUTION: "reset = true" removes existing persisted data!!!
reset = false
# Max number of the log entries in each persistent<T>, default to 1048576
max_log_entry = 1048576
# Max data size in bytes for each persistent<T>, default to 512GB
max_data_size = 549755813888

# Logger configurations
[LOGGER]
# default log name
default_log_name = derecho_debug
# default log level
# Available options:
# trace,debug,info,warn,error,critical,off
default_log_level = trace

# cascade service configuration
# TODO: add document for how to setup a cascade service.
[CASCADE]
# Cascade server allows application-defined behavior. The behavior is divided into two parts: one is on-critical data
# path, the other is off-critical data path. The behavior API is defined in <cascade/service_server_api.hpp>. An
# application using this feature needs to implement that API and create a shared library. And then tell the server
# where to find it by "ondata_library". We show a reference implementation in cascade source code:
#     <cascade_source>/src/service/data_path_logic/ondata_library_exmaple.cpp
# "ondata_library" is defaulted to empty
# Please note that the extra options for a given ondata library are also specified here. For example, the cnn_classifier
# library ('libcnn_classifier_dpl.so') requires locations for its models
# flower_synset =
# flower_symbol =
# flower_params =
# pet_synset =
# pet_symbol =
# pet_params =
ondata_library = 
# Specify group layout here. The layout specifies the following items:
# - How many subgroups of corresponding type (see note below) to create?
# - For each subgroup, how many shards to create?
# - For each shard, what are the minimum number of nodes required(min_nodes_by_shard, defaulted to 1), the maximum
#   number of nodes allowed(max_nodes_by_shard, defaulted to 1), the delivery mode(delivery_modes_by_shard, either
#   "Ordered" or "Raw", defaulted to "Ordered"), and the profile name(profiles_by_shard, defaulted to "DEFAULT")
# Derecho parameters in "[SUBGROUP/<profile>]" will be used for the corresponding shard.
# 
# The setup is defined in a json array, where each element is a dictionary specifying the layout for a corresponding
# subgroup type. OK, I mentioned "corresponding subgroup type" again and here is the mapping between the configuration
# elements and types used to define a Cascade service --- in the cascade service server code, we started a derecho
# group with a list of types, which are currently given as follows:
# - VolatileCascadeStoreWithStringKey
# - PersistentCascadeStoreWithStringKey
# Each of the type in the list corresponds to an entry in the layout json array defined here, following the order in
# the type list. Therefore, with the current type list setup, the layout has four elements with the 1-st for type
# "VolatileCascadeStoreWithStringKey", and the 2-nd for type "PersistentCascadeStoreWithStringKey".
# An element whose index is greater than the number of types is simply ignored.
# 
# Each dictionary element has two keys: "type_alias" and "layout". The "type_alias" specifies the human-readable name
# (string) for the corresponding (sigh...the first stressless "corresponding") type. The "layout" define, with a json 
# array, the setup of subgroups of this type. Each element (a layout dict) for a subgroup. Each layout dict has four
# entries corresponding to the above four items. The value for each entry is an array whose length is equal to the
# number of shards. Each element in that array is a setting for the corresponding shard.
# For example, the following configuration defined two subgroups of VolatileCascadeStoreWithStringKey type. The first
# subgroup has one shard and the second subgroup has three shards
#
# group_layout = '
# [
#     {
#         "type_alias":  "VCSS",
#         "layout":     [
#                           {
#                               "min_nodes_by_shard": [1],
#                               "max_nodes_by_shard": [1],
#                               "delivery_modes_by_shard": ["Ordered"],
#                               "profiles_by_shard": ["DEFAULT"]
#                           },
#                           {
#                               "min_nodes_by_shard": [1,3,5],
#                               "max_nodes_by_shard": [3,5,7],
#                               "delivery_modes_by_shard": ["Ordered","Ordered","Raw"],
#                               "profiles_by_shard": ["VCSS_SHARD1","VCSS_SHARD2","VCSS_SHARD3"]
#                           }
#                       ]
#     },
#     { ... },
#     { ... },
#     { ... }
#
# ]
# '
# Please make sure a pair of single quotation marks ' are used around the json. GetPot formation enforced that for
# multiline values.
group_layout = ''

# TODO: specify the resource allocation for the cascade context. For exmaple:
# cpu_cores = 0,1,2,3,6,7
# mem_cap_gibyte = 16
# Those can be applied by cgroups.
# gpus = 0,1

# number of off critical path threads default to 1
# TODO: in the future, this should be more flexible
num_off_critical_data_path_threads = 1
