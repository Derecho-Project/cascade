[DERECHO]
# contact ip - the current leader's ip address
contact_ip = 127.0.0.1
# contact port - the current leader's gms port
contact_port = 23580
# my local id - each node should have a different id
local_id = 0
# my local ip address
local_ip = 127.0.0.1
# derecho gms port
gms_port = 23580
# derecho rpc port
state_transfer_port = 28366
# sst tcp port
sst_port = 37683
# rdmc tcp port
rdmc_port = 31675
# external port
external_port = 32645
# this is the frequency of the failure detector thread.
# It is best to leave this to 1 ms for RDMA. If it is too high,
# you run the risk of overflowing the queue of outstanding sends.
heartbeat_ms = 100
# sst poll completion queue timeout in millisecond
sst_poll_cq_timeout_ms = 100
# disable partitioning safety
# By disabling this feature, the derecho is allowed to run when active
# members cannot form a majority. Please be aware of the 'split-brain'
# syndrome:https://en.wikipedia.org/wiki/Split-brain and make sure your
# application is fine with it. 
# To help the user play with derecho at beginning, we disabled the
# partitioning safety. We suggest to set it to false for serious deployment
disable_partitioning_safety = false

# maximum payload size for P2P requests
max_p2p_request_payload_size = 10240
# maximum payload size for P2P replies
max_p2p_reply_payload_size = 10240
# window size for P2P requests and replies
p2p_window_size = 16

# Subgroup configurations
# - The default subgroup settings
[SUBGROUP/DEFAULT]
# maximum payload size
# Any message with size large than this has to be broken
# down to multiple messages.
# Large message consumes memory space because the memory buffers
# have to be pre-allocated.
max_payload_size = 10240
# maximum reply payload size
# This is for replies generated by ordered sends in the subgroup
max_reply_payload_size = 10240
# maximum smc (SST's small message multicast) payload size
# If the message size is smaller or equal to this size,
# it will be sent using SST multicast, otherwise it will
# try RDMC if the message size is smaller than max_payload_size.
max_smc_payload_size = 10240
# block size depends on your max_payload_size.
# It is only relevant if you are ever going to send a message using RDMC.
# In that case, it should be set to the same value as the max_payload_size,
# if the max_payload_size is around 1 MB. For very large messages, the block # size should be a few MBs (1 is fine).
block_size = 1048576
# message window size
# the length of the message pipeline
window_size = 16
# the send algorithm for RDMC. Other options are
# chain_send, sequential_send, tree_send
rdmc_send_algorithm = binomial_send
# - SAMPLE for large message settings
[SUBGROUP/LARGE]
max_payload_size = 102400
max_reply_payload_size = 102400
max_smc_payload_size = 10240
block_size = 10240
window_size = 3
rdmc_send_algorithm = binomial_send
# - SAMPLE for small message settings
[SUBGROUP/SMALL]
max_payload_size = 100
max_reply_payload_size = 100
max_smc_payload_size = 100
# does not matter unless max_payload_size > max_smc_payload_size
block_size = 0
window_size = 50
rdmc_send_algorithm = binomial_send

[SUBGROUP/VCS]
max_payload_size = 8192
max_reply_payload_size = 8192
max_smc_payload_size = 10240
# does not matter unless max_payload_size > max_smc_payload_size
block_size = 1048576
window_size = 50
rdmc_send_algorithm = binomial_send
num_shards = 1
min_nodes = 1
max_nodes = 4

[SUBGROUP/PCS]
max_payload_size = 8192
max_reply_payload_size = 8192
max_smc_payload_size = 10240
# does not matter unless max_payload_size > max_smc_payload_size
block_size = 1048576
window_size = 50
rdmc_send_algorithm = binomial_send
num_shards = 1
min_nodes = 1
max_nodes = 4

# RDMA section contains configurations of the following
# - which RDMA device to use
# - device configurations
[RDMA]
# 1. provider = bgq|gni|mlx|netdir|psm|psm2|rxd|rxm|shm|tcp|udp|usnic|verbs
# possible options(only 'tcp' and 'verbs' providers are tested so far):
# bgq     - The Blue Gene/Q Fabric Provider
# gni     - The GNI Fabric Provider (Cray XC (TM) systems)
# mlx     - The MLX Fabric Provider (UCX library)
# netdir  - The Network Direct Fabric Provider (Microsoft Network Direct SPI)
# psm     - The PSM Fabric Provider
# psm2    - The PSM2 Fabric Provider
# rxd     - The RxD (RDM over DGRAM) Utility Provider
# rxm     - The RxM (RDM over MSG) Utility Provider
# shm     - The SHM Fabric Provider
# tcp - The Sockets Fabric Provider (TCP)
# udp     - The UDP Fabric Provider
# usnic   - The usNIC Fabric Provider (Cisco VIC)
# verbs   - The Verbs Fabric Provider
provider = tcp

# 2. domain
# For tcp provider, domain is the NIC name (ifconfig | grep -v -e "^ ")
# For verbs provider, domain is the device name (ibv_devices)
domain = lo

# 3. tx_depth 
# tx_depth applies to hints->tx_attr->size, where hint is a struct fi_info object.
# see https://ofiwg.github.io/libfabric/master/man/fi_getinfo.3.html
tx_depth = 256

# 4. rx_depth:
# rx_depth applies to hints->rx_attr->size, where hint is a struct fi_info object.
# see https://ofiwg.github.io/libfabric/master/man/fi_getinfo.3.html
rx_depth = 256

# Persistent configurations
[PERS]
# persistent directory for file system-based logfile.
file_path = .plog
ramdisk_path = /dev/shm/volatile_t
# Reset persistent data
# CAUTION: "reset = true" removes existing persisted data!!!
reset = false
# Max number of the log entries in each persistent<T>, default to 1048576
max_log_entry = 1048576
# Max data size in bytes for each persistent<T>, default to 512GB
max_data_size = 549755813888

# Logger configurations
[LOGGER]
# default log name
default_log_name = derecho_debug
# default log level
# Available options:
# trace,debug,info,warn,error,critical,off
default_log_level = trace

[LAYOUT]
json_layout = '
[
    {
        "type_alias":   "CascadeMetadataService",
        "layout":       [
                            {
                                "min_nodes_by_shard": ["1"],
                                "max_nodes_by_shard": ["1"],
                                "delivery_modes_by_shard": ["Ordered"],
                                "profiles_by_shard": ["DEFAULT"]
                            }
                        ]
    },
    {
        "type_alias":   "VolatileCascadeStoreWithStringKey",
        "layout":       [
                            {
                                "min_nodes_by_shard": ["1"],
                                "max_nodes_by_shard": ["1"],
                                "delivery_modes_by_shard": ["Ordered"],
                                "profiles_by_shard": ["DEFAULT"]
                            }
                        ]
    },
    {
        "type_alias":   "PersistentCascadeStoreWithStringKey",
        "layout":       [
                            {
                                "min_nodes_by_shard": ["1","1"],
                                "max_nodes_by_shard": ["1","1"],
                                "delivery_modes_by_shard": ["Ordered","Ordered"],
                                "profiles_by_shard": ["DEFAULT","DEFAULT"]
                            },
                            {
                                "min_nodes_by_shard": ["1"],
                                "max_nodes_by_shard": ["1"],
                                "delivery_modes_by_shard": ["Ordered"],
                                "profiles_by_shard": ["DEFAULT"]
                            }
                        ]
    },
    {
        "type_alias":   "TriggerCascadeNoStoreWithStringKey",
        "layout":       [
                            {
                                "min_nodes_by_shard": ["1"],
                                "max_nodes_by_shard": ["1"],
                                "delivery_modes_by_shard": ["Ordered"],
                                "profiles_by_shard": ["DEFAULT"]
                            }
                        ]
    }
]'

# cascade service configuration
[CASCADE]
# TODO: specify the resource allocation for the cascade context. For exmaple:
cpu_cores = 0,1,2,3,6,7
gpus = 0,1
# We have four worker thread pools: 
#
# 1) two for the off critical data path threads for multicast, which process actions from delivery on Derecho's predicate
#    thread; and
# 2) two for the off critical data path threads for p2p, which process actions from p2p message handler thread.
# Stateful/stateless: if a UDL is registered as stateful, the same key will always be handled in the same thread. Otherwise,
# the messages corresponding to the same key might be handled by different threads. We separated the thread pool for
# stateful and stateless handling.
#
# The default number of threads in multicast ocdp pool is 1
num_stateless_workers_for_multicast_ocdp = 1
num_stateful_workers_for_multicast_ocdp = 1
# The default number of threads in p2p send ocdp pool is 1
num_stateless_workers_for_p2p_ocdp = 1
num_stateful_workers_for_p2p_ocdp = 1

# Specify the worker affinity to CPU cores.
# The format of the worker affinity is in json. The keys are thread number (0 to `num_workers-1`).
# The values are dicts describing the resources attached to this resource. Currently, we support only CPU resource.
# Please note that the keys does not necessarily cover all threads. Unspecified threads are scheduled up to CPU scheduler.
# worker_cpu_affinity = '
# {
#   "multicast_ocdp": {
#                         "0": "0,1",
#                         "1": "4,5"
#                     },
#   "p2p_ocdp":       {
#                         "0": "2,3",
#                         "1": "6,7"
#                     }
# }
# '
# TODO: currently, we only set the cpu core affinity using pthread_setaffinity(). The threads created in the worker
# thread derive the cpu affinity automatically. But the affinity is NOT enforced, meaning the thread/child threads
# can overwrite the preset affinity. For example, MXNet CPU context will use all CPU cores available to the Cascade
# Server process. In the future, we should enforce this later.
worker_cpu_affinity = 

# timestamp tag filter is used to control which timestamp tags to log. The timestamp tags are defined in 
# `include/cascade/utils.hpp`. timestamp_tag_enabler lists the set of tags that will be logged in the system, separated
# by ','. For example, the following filter will log TLT_VOLATILE_PUT_START and TLT_VOLATILE_PUT_END
timestamp_tag_enabler = 2,3
