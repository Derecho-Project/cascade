[DERECHO]
# leader ip - the leader's ip address
leader_ip = 127.0.0.1
# leader gms port - the leader's gms port
leader_gms_port = 23580
# leader external client port - the leader's 
leader_external_port = 32645
# my local id - each node should have a different id
local_id = 0
# my local ip address
local_ip = 127.0.0.1
# derecho gms port
gms_port = 23580
# derecho rpc port
state_transfer_port = 28366
# sst tcp port
sst_port = 37683
# rdmc tcp port
rdmc_port = 31675
# external port
external_port = 32645
# this is the frequency of the failure detector thread.
# It is best to leave this to 1 ms for RDMA. If it is too high,
# you run the risk of overflowing the queue of outstanding sends.
heartbeat_ms = 100
# sst poll completion queue timeout in millisecond
sst_poll_cq_timeout_ms = 100
# disable partitioning safety
# By disabling this feature, the derecho is allowed to run when active
# members cannot form a majority. Please be aware of the 'split-brain'
# syndrome:https://en.wikipedia.org/wiki/Split-brain and make sure your
# application is fine with it. 
# To help the user play with derecho at beginning, we disabled the
# partitioning safety. We suggest to set it to false for serious deployment
disable_partitioning_safety = false

# maximum payload size for P2P requests
max_p2p_request_payload_size = 10240
# maximum payload size for P2P replies
max_p2p_reply_payload_size = 10240
# window size for P2P requests and replies
p2p_window_size = 16

# Subgroup configurations
# - The default subgroup settings
[SUBGROUP/DEFAULT]
# maximum payload size
# Any message with size large than this has to be broken
# down to multiple messages.
# Large message consumes memory space because the memory buffers
# have to be pre-allocated.
max_payload_size = 10240
# maximum reply payload size
# This is for replies generated by ordered sends in the subgroup
max_reply_payload_size = 10240
# maximum smc (SST's small message multicast) payload size
# If the message size is smaller or equal to this size,
# it will be sent using SST multicast, otherwise it will
# try RDMC if the message size is smaller than max_payload_size.
max_smc_payload_size = 10240
# block size depends on your max_payload_size.
# It is only relevant if you are ever going to send a message using RDMC.
# In that case, it should be set to the same value as the max_payload_size,
# if the max_payload_size is around 1 MB. For very large messages, the block # size should be a few MBs (1 is fine).
block_size = 1048576
# message window size
# the length of the message pipeline
window_size = 16
# the send algorithm for RDMC. Other options are
# chain_send, sequential_send, tree_send
rdmc_send_algorithm = binomial_send
# - SAMPLE for large message settings
[SUBGROUP/LARGE]
max_payload_size = 102400
max_reply_payload_size = 102400
max_smc_payload_size = 10240
block_size = 10240
window_size = 3
rdmc_send_algorithm = binomial_send
# - SAMPLE for small message settings
[SUBGROUP/SMALL]
max_payload_size = 100
max_reply_payload_size = 100
max_smc_payload_size = 100
# does not matter unless max_payload_size > max_smc_payload_size
block_size = 0
window_size = 50
rdmc_send_algorithm = binomial_send

[SUBGROUP/VCS]
max_payload_size = 8192
max_reply_payload_size = 8192
max_smc_payload_size = 10240
# does not matter unless max_payload_size > max_smc_payload_size
block_size = 1048576
window_size = 50
rdmc_send_algorithm = binomial_send
num_shards = 1
min_nodes = 1
max_nodes = 4

[SUBGROUP/PCS]
max_payload_size = 8192
max_reply_payload_size = 8192
max_smc_payload_size = 10240
# does not matter unless max_payload_size > max_smc_payload_size
block_size = 1048576
window_size = 50
rdmc_send_algorithm = binomial_send
num_shards = 1
min_nodes = 1
max_nodes = 4

# RDMA section contains configurations of the following
# - which RDMA device to use
# - device configurations
[RDMA]
# 1. provider = bgq|gni|mlx|netdir|psm|psm2|rxd|rxm|shm|sockets|udp|usnic|verbs
# possible options(only 'sockets' and 'verbs' providers are tested so far):
# bgq     - The Blue Gene/Q Fabric Provider
# gni     - The GNI Fabric Provider (Cray XC (TM) systems)
# mlx     - The MLX Fabric Provider (UCX library)
# netdir  - The Network Direct Fabric Provider (Microsoft Network Direct SPI)
# psm     - The PSM Fabric Provider
# psm2    - The PSM2 Fabric Provider
# rxd     - The RxD (RDM over DGRAM) Utility Provider
# rxm     - The RxM (RDM over MSG) Utility Provider
# shm     - The SHM Fabric Provider
# sockets - The Sockets Fabric Provider (TCP)
# udp     - The UDP Fabric Provider
# usnic   - The usNIC Fabric Provider (Cisco VIC)
# verbs   - The Verbs Fabric Provider
provider = sockets

# 2. domain
# For sockets provider, domain is the NIC name (ifconfig | grep -v -e "^ ")
# For verbs provider, domain is the device name (ibv_devices)
domain = lo

# 3. tx_depth 
# tx_depth applies to hints->tx_attr->size, where hint is a struct fi_info object.
# see https://ofiwg.github.io/libfabric/master/man/fi_getinfo.3.html
tx_depth = 256

# 4. rx_depth:
# rx_depth applies to hints->rx_attr->size, where hint is a struct fi_info object.
# see https://ofiwg.github.io/libfabric/master/man/fi_getinfo.3.html
rx_depth = 256

# Persistent configurations
[PERS]
# persistent directory for file system-based logfile.
file_path = .plog
ramdisk_path = /dev/shm/volatile_t
# Reset persistent data
# CAUTION: "reset = true" removes existing persisted data!!!
reset = false
# Max number of the log entries in each persistent<T>, default to 1048576
max_log_entry = 1048576
# Max data size in bytes for each persistent<T>, default to 512GB
max_data_size = 549755813888

# Logger configurations
[LOGGER]
# default log name
default_log_name = derecho_debug
# default log level
# Available options:
# trace,debug,info,warn,error,critical,off
default_log_level = trace

# cascade service configuration
[CASCADE]
# Cascade server allows application-defined behavior. The application can specify the data path logic corresponding to
# a set of objects identified by a $prefix$. The key of each objects in cascade is presumably type-convertible to a
# string like "/A/B/C/D/E", in which the string before the last path separator is considered the $prefix$ of the
# object. In our roadmap, the objects will be managed in a logical concept called $object pool$. All objects in an 
# object pool has the same prefix and no two object pools allow the same prefix. The data path logic design matches
# the object pool concept in that the objects in an object pool should be processed by the sample application-defined
# data path logic.
#
# The interface of the data path logic is defined in cascade/data_path_logic_interface.hpp. To create the data path
# logic, an application programmer requires to implement the three functions defined in the interface and compiles the
# implementation into a dynamic linked library (dll) file. Please refer to the console_printer_dpl.cpp example to see
# how to create a data path logic interface
# 
# TODO: explain the implementation of data_path_logic_manager and how the dll file is managed by it.
#
# Please note that the extra options for a given ondata library are also specified here. For example, the cnn_classifier
# library ('libcnn_classifier_dpl.so') requires locations for its models
# flower_synset =
# flower_symbol =
# flower_params =
# pet_synset =
# pet_symbol =
# pet_params =
# by default, cnn_classifier will use CPU, set use_gpu = true to use gpus. If multiple GPU exists, a worker will decide
# on which GPU to use by work_id (work_id % num_gpus)
# use_gpu = false

# Specify group layout here. The layout specifies the following items:
# - How many subgroups of corresponding type (see note below) to create?
# - For each subgroup, how many shards to create?
# - For each shard, what are the minimum number of nodes required(min_nodes_by_shard, defaulted to 1), the maximum
#   number of nodes allowed(max_nodes_by_shard, defaulted to 1), the delivery mode(delivery_modes_by_shard, either
#   "Ordered" or "Raw", defaulted to "Ordered"), and the profile name(profiles_by_shard, defaulted to "DEFAULT")
# Derecho parameters in "[SUBGROUP/<profile>]" will be used for the corresponding shard.
# 
# The setup is defined in a json array, where each element is a dictionary specifying the layout for a corresponding
# subgroup type. OK, I mentioned "corresponding subgroup type" again and here is the mapping between the configuration
# elements and types used to define a Cascade service --- in the cascade service server code, we started a derecho
# group with a list of types, which are currently given as follows:
# - VolatileCascadeStoreWithStringKey
# - PersistentCascadeStoreWithStringKey
# Each of the type in the list corresponds to an entry in the layout json array defined here, following the order in
# the type list. Therefore, with the current type list setup, the layout has four elements with the 1-st for type
# "VolatileCascadeStoreWithStringKey", and the 2-nd for type "PersistentCascadeStoreWithStringKey".
# An element whose index is greater than the number of types is simply ignored.
# 
# Each dictionary element has two keys: "type_alias" and "layout". The "type_alias" specifies the human-readable name
# (string) for the corresponding (sigh...the first stressless "corresponding") type. The "layout" define, with a json 
# array, the setup of subgroups of this type. Each element (a layout dict) for a subgroup. Each layout dict has four
# entries corresponding to the above four items. The value for each entry is an array whose length is equal to the
# number of shards. Each element in that array is a setting for the corresponding shard.
# For example, the following configuration defined two subgroups of VolatileCascadeStoreWithStringKey type. The first
# subgroup has one shard and the second subgroup has three shards
#
# group_layout = '
# [
#     {
#         "type_alias":  "VCSS",
#         "layout":     [
#                           {
#                               "min_nodes_by_shard": [1],
#                               "max_nodes_by_shard": [1],
#                               "delivery_modes_by_shard": ["Ordered"],
#                               "profiles_by_shard": ["DEFAULT"]
#                           },
#                           {
#                               "min_nodes_by_shard": [1,3,5],
#                               "max_nodes_by_shard": [3,5,7],
#                               "delivery_modes_by_shard": ["Ordered","Ordered","Raw"],
#                               "profiles_by_shard": ["VCSS_SHARD1","VCSS_SHARD2","VCSS_SHARD3"]
#                           }
#                       ]
#     },
#     { ... },
#     { ... },
#     { ... }
#
# ]
# '
# Please make sure a pair of single quotation marks ' are used around the json. GetPot formation enforced that for
# multiline values.
group_layout = ''

# TODO: specify the resource allocation for the cascade context. For exmaple:
cpu_cores = 0,1,2,3,6,7
gpus = 0,1

# number of off critical path workers, default to 1
num_workers = 1

# Specify the worker affinity to CPU cores.
# The format of the worker affinity is in json. The keys are thread number (0 to `num_workers-1`).
# The values are dicts describing the resources attached to this resource. Currently, we support only CPU resource.
# Please note that the keys does not necessarily cover all threads. Unspecified threads are scheduled up to CPU scheduler.
# worker_cpu_affinity = '
# {
#   "0": {"cpu_cores":"0,1,2,3"},
#   "1": {"cpu_cores":"4,5,6,7"}
# }
# '
# TODO: currently, we only set the cpu core affinity using pthread_setaffinity(). The threads created in the worker
# thread derive the cpu affinity automatically. But the affinity is NOT enforced, meaning the thread/child threads
# can overwrite the preset affinity. For example, MXNet CPU context will use all CPU cores available to the Cascade
# Server process. In the future, we should enforce this later.
worker_cpu_affinity = 
